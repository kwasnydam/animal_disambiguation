{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the feature extraction will be performed. For the first prototype I am going to performs following transformations:\n",
    "1. word tokenize\n",
    "2. decapitalize\n",
    "3. remove stopwords\n",
    "\n",
    "This is my tokenization sequence performed on input string\n",
    "\n",
    "Once I have a suitable set of tokens, I am going to compute tfidf for the training set. I want to use the tfidf generated on train set on validation and test sets later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48519\\Professional Stuff\\various\\machine_learning\\mouse_disambiguation\n",
      "['mouse', 'small', 'rodent']\n"
     ]
    }
   ],
   "source": [
    "# build a tokenizer\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "get_dir = os.path.dirname\n",
    "PROJ_ROOT = get_dir(get_dir(os.path.abspath('__file__')))\n",
    "print(PROJ_ROOT)\n",
    "sys.path.append(os.path.join(PROJ_ROOT, 'src'))\n",
    "\n",
    "from data import dataset\n",
    " \n",
    "stop_words = stopwords.words('english')\n",
    " \n",
    "def tokenize(text):\n",
    "    \"\"\"Perform tokenization of input sentence\n",
    "    \n",
    "    Arguments:\n",
    "        text: string representing a single sentence.\n",
    "    \n",
    "    Output:\n",
    "        List of tokens\n",
    "        \n",
    "    First, using nltk word_tokenize splits the sentence into tokens\n",
    "    Then, lowercases all tokens\n",
    "    Finally, removes stopwords tokens and digits\n",
    "    return a list of valid tokens\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    words = [w.lower() for w in words]\n",
    "    return [w for w in words if w not in stop_words and not w.isdigit()]\n",
    "\n",
    "# let's test our function\n",
    "test_sentence = 'Mouse is a small rodent'\n",
    "expected_output = ['mouse', 'small', 'rodent']\n",
    "assert expected_output == tokenize(test_sentence)\n",
    "print(tokenize(test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "553\n",
      "  (0, 349)\t0.4123504493344321\n",
      "  (0, 295)\t0.4123504493344321\n",
      "  (0, 187)\t0.4123504493344321\n",
      "  (0, 186)\t0.3702289282010234\n",
      "  (0, 185)\t0.3702289282010234\n",
      "  (0, 161)\t0.464502316729765\n",
      "  (1, 535)\t0.45457813663961644\n",
      "  (1, 315)\t0.5416259942887288\n",
      "  (1, 235)\t0.45457813663961644\n",
      "  (1, 64)\t0.5416259942887288\n",
      "['1980s', '3byte', '3byte packets', 'ability', 'able', 'absolute', 'acceleration', 'accessory', 'actions', 'adjustable', 'aiming', 'allow', 'almost', 'along', 'also', 'alternative', 'alternative actions', 'alto', 'america', 'amiga', 'amiga atari', 'amiga atari st', 'apodemus', 'apparently', 'applications', 'arid', 'around', 'array', 'atari', 'atari st', 'available', 'axes', 'axis', 'backward', 'ball', 'ball mouse', 'baney', 'based', 'batteries', 'battery', 'beams', 'became', 'become', 'bill', 'bill english', 'birds', 'birds prey', 'breeding', 'bring', 'burrows', 'bus', 'button', 'button click', 'button mouse', 'buttons', 'called', 'came', 'cats', 'cause', 'change', 'china', 'china north', 'china north america', 'click', 'combination', 'come', 'commercial', 'common', 'commonly', 'communication', 'company', 'compatible', 'component', 'computer', 'computer mouse', 'computers', 'configuration', 'connected', 'connection', 'connector', 'consumer', 'continuously', 'controlling', 'controls', 'could', 'cpi', 'credited', 'cross', 'cursor', 'cursor moves', 'dark', 'default', 'delete', 'derived', 'design', 'designed', 'designed use', 'desk', 'detect', 'detect movement', 'detected', 'detects', 'development', 'device', 'devices', 'diet', 'different', 'diode', 'dipodid', 'dipodid rodent', 'dipodid rodent native', 'direction', 'direction often', 'directly', 'distance', 'dogs', 'douglas', 'downward', 'dpi', 'drag', 'driver', 'driver software', 'due', 'dunes', 'earliest', 'early', 'early mouse', 'early optical', 'early optical mouse', 'easily', 'either', 'electrical', 'emitter', 'employ', 'encoder', 'engelbart', 'engelbarts', 'engelbarts original', 'engelbarts original mouse', 'english', 'ergonomic', 'ergonomic mouse', 'european', 'even', 'even mouse', 'example', 'expressed', 'extinct', 'far', 'far mouse', 'fast', 'faster', 'features', 'field', 'first', 'first mouse', 'firstperson', 'firstperson shooter', 'food', 'form', 'forward', 'found', 'function', 'game', 'games', 'gaming', 'gaming mouse', 'generally', 'genus', 'glass', 'glossy', 'glossy transparent', 'glossy transparent surfaces', 'grip', 'gui', 'hand', 'hands', 'hardware', 'hawley', 'held', 'high', 'higher', 'highresolution', 'holding', 'holes', 'hopping', 'hopping mouse', 'hopping mouse notomys', 'hoppingmouse', 'house', 'however', 'however laser', 'however laser mouse', 'however mouse', 'ibm', 'illumination', 'image', 'image offset', 'image offset previous', 'images', 'important', 'improvement', 'inch', 'including', 'inertial', 'inexpensive', 'infrared', 'input', 'inputs', 'inside', 'instead', 'integrated', 'intended', 'interface', 'introduced', 'invention', 'inverted', 'jack', 'jack hawley', 'joystick', 'jumping', 'jumping mouse', 'kangaroo', 'kangaroo mouse', 'key', 'keyboard', 'known', 'large', 'largely', 'laser', 'laser mouse', 'lasers', 'lead', 'led', 'leds', 'left', 'left right', 'lefthanded', 'light', 'like', 'limited', 'lisa', 'list', 'lizards', 'logitech', 'look', 'low', 'low power', 'macintosh', 'main', 'make', 'make mouse', 'manufactured', 'manufacturers', 'many', 'market', 'marketed', 'may', 'means', 'mechanical', 'mechanical mouse', 'menu', 'menu alternative', 'menu alternative actions', 'mice', 'mickey', 'mickeys', 'microsoft', 'microsofts', 'middle', 'might', 'mode', 'modern', 'modern mouse', 'modern optical', 'modern optical mouse', 'mother', 'motion', 'motion mouse', 'mouse apodemus', 'mouse button', 'mouse cause', 'mouse cursor', 'mouse detect', 'mouse driver', 'mouse either', 'mouse genus', 'mouse house', 'mouse input', 'mouse instead', 'mouse largely', 'mouse may', 'mouse moved', 'mouse movement', 'mouse moving', 'mouse notomys', 'mouse often', 'mouse pad', 'mouse pads', 'mouse plural', 'mouse plural mouse', 'mouse require', 'mouse required', 'mouse sensitivity', 'mouse shipped', 'mouse small', 'mouse small rodent', 'mouse sold', 'mouse species', 'mouse specifically', 'mouse starting', 'mouse two', 'mouse use', 'mouse used', 'mouse uses', 'mouse wheel', 'mouse work', 'mouse would', 'mouselike', 'mousepad', 'mouses', 'move', 'moved', 'movement', 'movement mouse', 'movements', 'moves', 'moves mouse', 'moving', 'moving mouse', 'moving parts', 'much', 'multiple', 'named', 'native', 'native china', 'native china north', 'navigation', 'nearly', 'need', 'never', 'new', 'nm', 'nondesertdwelling', 'nondesertdwelling dipodid', 'nondesertdwelling dipodid rodent', 'north', 'north america', 'notable', 'notomys', 'object', 'offer', 'offset', 'offset previous', 'offset previous one', 'often', 'one', 'onto', 'open', 'operating', 'opposite', 'optical', 'optical laser', 'optical laser mouse', 'optical mouse', 'optical mouse use', 'optical mouse work', 'optional', 'original', 'original mouse', 'originally', 'packets', 'pad', 'pads', 'palm', 'palm rests', 'part', 'parts', 'pc', 'per', 'per inch', 'performance', 'perpendicular', 'personal', 'pet', 'pets', 'photodiodes', 'photodiodes detect', 'pixel', 'pixels', 'platforms', 'players', 'plural', 'plural mouse', 'plural mouse small', 'pointer', 'popular', 'position', 'potentiometers', 'power', 'power requirements', 'powersaving', 'predators', 'previous', 'previous one', 'prey', 'primary', 'produced', 'provide', 'ps2', 'ps2 mouse', 'quadratureencoded', 'quadratureencoded x', 'quickly', 'rats', 'receivers', 'red', 'refer', 'referred', 'regarded', 'relative', 'relative surface', 'relatively', 'replaced', 'report', 'reported', 'require', 'required', 'requirements', 'requires', 'rests', 'result', 'right', 'right mouse', 'rodent', 'rodent native', 'rodent native china', 'roll', 'roller', 'rolling', 'rollkugel', 'sand', 'sand dunes', 'screen', 'sensitivity', 'sensor', 'sensors', 'serial', 'several', 'shape', 'shipped', 'shooter', 'signals', 'similar', 'single', 'small', 'small rodent', 'snakes', 'snakes lizards', 'software', 'sold', 'sometimes', 'special', 'species', 'specifically', 'speed', 'speed movement', 'st', 'standard', 'star', 'starting', 'state', 'states', 'still', 'successive', 'surface', 'surfaces', 'switch', 'system', 'systems', 'tactile', 'tactile mouse', 'take', 'team', 'technologies', 'telefunken', 'terms', 'third', 'though', 'three', 'throughout', 'thumb', 'thus', 'time', 'touch', 'track', 'track glossy', 'track glossy transparent', 'trackball', 'tracking', 'traditional', 'traditional mouse', 'transparent', 'transparent surfaces', 'turn', 'two', 'two mouse', 'type', 'type mechanical', 'type mechanical mouse', 'types', 'typically', 'understand', 'usb', 'use', 'use image', 'use mouse', 'used', 'used mouse', 'user', 'users', 'uses', 'using', 'using mouse', 'usually', 'varied', 'variety', 'various', 'vcsel', 'vectors', 'version', 'versus', 'vertical', 'video', 'well', 'wheel', 'wheels', 'whose', 'wide', 'widely', 'wired', 'wireless', 'wireless mouse', 'without', 'wood', 'work', 'working', 'would', 'x', 'xerox', 'xerox alto', 'xerox star', 'â€“']\n"
     ]
    }
   ],
   "source": [
    "# Now, since the tokenizer is ready we can build our vectorizer.\n",
    "\n",
    "READ_DIRECTORY = dataset.DEFAULT_PROCESSED_TEXT_DATA_DIRECTORY\n",
    "train_dataset_filepath = os.path.join(READ_DIRECTORY, 'train.csv')\n",
    "train_dataset = pd.read_csv(train_dataset_filepath, sep=';')\n",
    "train_dataset.head()\n",
    "\n",
    "vectorization_parameters ={\n",
    "    'ngrams': (1, 3),\n",
    "    'min_df': 2,\n",
    "    'max_df': 0.5\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=tokenize,\n",
    "    max_df=vectorization_parameters['max_df'],\n",
    "    min_df=vectorization_parameters['min_df'],\n",
    "    ngram_range=vectorization_parameters['ngrams']\n",
    ")\n",
    "\n",
    "vectorizer.fit(train_dataset.iloc[:,0]) # train on variables from train set (dont pass classes)\n",
    "features = vectorizer.transform(train_dataset.iloc[:,0].copy())\n",
    "print(type(features))\n",
    "print(len(vectorizer.idf_))\n",
    "print(features[0:2])\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is left now is to save the generated features as well as to save the parameters of the vectorizer. \n",
    "It will be used at evaluation to tranform the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1\n",
      " 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "FEATURES_SAVE_DIRECTORY = os.path.join(dataset.DEFAULT_PROCESSED_DATA_DIRECTORY, 'features')\n",
    "\n",
    "def name_from_configuration(configuration):\n",
    "    return 'ngrams_{ngrams}_maxdf_{max_df}_min_df_{min_df}'.format(**configuration)\n",
    "\n",
    "# we can perform the vectorization with different parameters, so it is important to keep track of it somehow\n",
    "transformation_name = name_from_configuration(vectorization_parameters)\n",
    "transformation_name\n",
    "\n",
    "FEATURES_SAVE_DIRECTORY = os.path.join(FEATURES_SAVE_DIRECTORY, transformation_name)\n",
    "DATA_MODEL_SAVE_DIRECTORY = os.path.join(PROJ_ROOT, 'models', 'data', transformation_name)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LE = LabelEncoder()\n",
    "classes_binarized = LE.fit_transform(train_dataset.iloc[:,1])\n",
    "print(classes_binarized)\n",
    "train_dataset = np.hstack(features.todense())\n",
    "\n",
    "features = pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=vectorizer.get_feature_names(),\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
