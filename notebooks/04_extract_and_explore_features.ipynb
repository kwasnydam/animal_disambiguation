{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the feature extraction will be performed. For the first prototype I am going to performs following transformations:\n",
    "1. word tokenize\n",
    "2. decapitalize\n",
    "3. remove stopwords\n",
    "\n",
    "This is my tokenization sequence performed on input string\n",
    "\n",
    "Once I have a suitable set of tokens, I am going to compute tfidf for the training set. I want to use the tfidf generated on train set on validation and test sets later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48519\\Professional Stuff\\various\\machine_learning\\mouse_disambiguation\n",
      "['mouse', 'small', 'rodent']\n"
     ]
    }
   ],
   "source": [
    "# build a tokenizer\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "get_dir = os.path.dirname\n",
    "PROJ_ROOT = get_dir(get_dir(os.path.abspath('__file__')))\n",
    "print(PROJ_ROOT)\n",
    "sys.path.append(os.path.join(PROJ_ROOT, 'src'))\n",
    "\n",
    "from data import dataset\n",
    " \n",
    "stop_words = stopwords.words('english')\n",
    " \n",
    "def tokenize(text):\n",
    "    \"\"\"Perform tokenization of input sentence\n",
    "    \n",
    "    Arguments:\n",
    "        text: string representing a single sentence.\n",
    "    \n",
    "    Output:\n",
    "        List of tokens\n",
    "        \n",
    "    First, using nltk word_tokenize splits the sentence into tokens\n",
    "    Then, lowercases all tokens\n",
    "    Finally, removes stopwords tokens and digits\n",
    "    return a list of valid tokens\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    words = [w.lower() for w in words]\n",
    "    return [w for w in words if w not in stop_words and not w.isdigit()]\n",
    "\n",
    "# let's test our function\n",
    "test_sentence = 'Mouse is a small rodent'\n",
    "expected_output = ['mouse', 'small', 'rodent']\n",
    "assert expected_output == tokenize(test_sentence)\n",
    "print(tokenize(test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('device', 160), ('animal', 54)]\n",
      "['animal', 'device', 'device', 'device', 'animal'] [1 0 0 0 1]\n",
      "{'label_encoder': {'labels': {'device': 0, 'animal': 1}, 'inverse_mapping': {'0': 'device', '1': 'animal'}}, 'vectorizer': {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.float64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 0.5, 'max_features': None, 'min_df': 2, 'ngram_range': (1, 3), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': <bound method TextLabelsVectorizer.tokenize of <data.dataset.TextLabelsVectorizer object at 0x056865F0>>, 'use_idf': True, 'vocabulary': None}}\n"
     ]
    }
   ],
   "source": [
    "# Now, since the tokenizer is ready we can build our vectorizer.\n",
    "\n",
    "READ_DIRECTORY = dataset.DEFAULT_PROCESSED_TEXT_DATA_DIRECTORY\n",
    "train_dataset_filepath = os.path.join(READ_DIRECTORY, 'train.csv')\n",
    "train_dataset = pd.read_csv(train_dataset_filepath, sep=';')\n",
    "train_dataset.head()\n",
    "\n",
    "vectorization_parameters ={\n",
    "    'ngrams': (1, 3),\n",
    "    'min_df': 2,\n",
    "    'max_df': 0.5\n",
    "}\n",
    "\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     analyzer='word',\n",
    "#     tokenizer=tokenize,\n",
    "#     max_df=vectorization_parameters['max_df'],\n",
    "#     min_df=vectorization_parameters['min_df'],\n",
    "#     ngram_range=vectorization_parameters['ngrams']\n",
    "# )\n",
    "\n",
    "vectorizer = dataset.TextLabelsVectorizer(dataset.DEFAULT_VECTORIZER_SETTINGS)\n",
    "\n",
    "vectorizer.fit(train_dataset.iloc[:,0], train_dataset.iloc[:,1]) \n",
    "features, classes = vectorizer.transform(train_dataset.iloc[:,0], train_dataset.iloc[:,1]) \n",
    "inverse_classes = vectorizer.get_classes_name(classes)\n",
    "print(inverse_classes[:5], classes[:5])\n",
    "print(vectorizer.get_params())\n",
    "# print(type(features))\n",
    "# print(features[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is left now is to save the generated features as well as to save the parameters of the vectorizer. \n",
    "It will be used at evaluation to tranform the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_SAVE_DIRECTORY = os.path.join(dataset.DEFAULT_PROCESSED_DATA_DIRECTORY, 'features')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
